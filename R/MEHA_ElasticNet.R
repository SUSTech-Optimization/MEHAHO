#' Solving elastic net hyper-parameter selection problem based on MEHA
#' @description This R function is written to solve the hyper-parameter selection problem of sparse group lasso using MEHA,
#'     please refer to the listed literature for the specific algorithm and model. In this case, the 2-norm term (which is smooth) of lower-level objective function is divided into \eqn{g(x,y)} in the problem studied by MEHA.
#' @references Liu, R., Liu, Z., Yao, W., Zeng, S., & Zhang, J. (2024).
#'     "Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy."
#'     Available at: https://openreview.net/forum?id=i6EtCiIK4a
#'
#'     Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
#'     "Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
#'     Available at: https://proceedings.mlr.press/v162/gao22j.html
#' @param A_val Input feature matrix of the validation set, with dimensions n by p,
#'     where n is the total number of validation samples and p is the dimension of
#'     features. Each row represents an observation vector.
#' @param b_val Quantitative response variable of validation set.
#' @param A_tr Input feature matrix of training set, with dimension n' by p,
#'     where n' is the total number of training samples and p is the dimension of features.
#' @param b_tr Quantitative response variable of training set.
#' @param N Total iterations of MEHA. Default is 500.
#' @param alpha Projection stepsize of \code{x}, which is fixed. Default is 0.001.
#'     Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.
#' @param beta  Proximal gradient stepsize of \code{y} which is fixed. Default is 1e-5.
#'     Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.
#' @param eta Proximal gradient stepsize of \code{theta}, which is fixed.
#'     Default is 1e-5.
#' @param gamma Moreau envelope parameter. Default is 2.
#' @param c \eqn{\underline{c}} in MEHA, which is used to generate the penalty parameter \eqn{c_k} by \eqn{c_k = \underline{c}(k+1)^{power}}.
#'     Default is \code{1e-2}.
#' @param power The power exponent in \eqn{c_k = \underline{c}(k+1)^{power}}. Default is 0.1.
#' @param tol Tolerance. IF \eqn{|(x^{k + 1}, y^{k + 1}) - (x^{k}, y^k)|/\sqrt{1+|(x^{k}, y^k)|^2} < tol},
#'     then terminate the iteration, where \eqn{x^k} represents the upper-level variable, \eqn{y^k} represents the lower-level variable.
#'     Default is 0.05.
#' @param auto_tuning When alpha, beta, eta are fixed, whether an auto-hyperparameter-tuning is needed.
#'     Default is \code{FALSE}.
#' @param temperature Temperature of simulating annealing method for auto-
#'     hyperparameter-tuning. Default is 0.1.
#'
#' @return
#'
#'   \item{x}{The first value is \code{x1} (Lasso penalty strength), while the
#'    second value is \code{x2} (Ridge penalty strength).}
#'   \item{y}{The feature coefficient vector, of dimension p, where p is the
#'       p is the dimension of features.}
#'   \item{X_seq}{A list. Describe the sequence x generated by MEHA in the iterative process.}
#'   \item{Y_seq}{A list. Describe the sequence y generated by MEHA in the iterative process.}
#'   \item{F_seq}{The upper function value sequence generated in the iterative process based on validation set.}
#'
#'
#' @export


MEHA_Elasticnet = function(A_val, b_val, A_tr, b_tr, N = 500, alpha = 1e-3, beta = 1e-5, eta = 1e-5, gamma = 2, c = 1e-2, power = 0.1, tol = 0.05, auto_tuning = FALSE, temperature = 0.1){

  library(progress)
  library(truncnorm)


  main_fun <- function(A_val, b_val, A_tr, b_tr, N, alpha, beta, eta, gamma = gamma, c = c, power = power, tol = tol){
    p_fea = dim(A_val)[2]



    x = matrix(rep(0), nrow = 2)
    x[1, ] = 1
    x[2, ] = 0.5 # Initialize x
    y = matrix(rep(0), nrow = p_fea) # Initialize feature coefficients
    theta = y
    # theta = matrix(rep(0), nrow = p_fea)
    e2 = matrix(rep(0), nrow = 2)
    ep = matrix(rep(0), nrow = p_fea)

    # Upper objective function
    F = function(x,y){
      result = 0.5*norm(A_val %*% y - b_val, type = "2")^2
      return(result)
    }

    lower_fun = function(x, y){
      result = 0.5*norm(A_tr %*%  y - b_tr, type = "2")^2 + cbind(norm(y, type = "1"), 0.5*norm(y,type = "2")^2 ) %*% x
      return(result)
    }


    # Gradient update function for x and y
    F_x = function(x, y){
      result = t(matrix(rep(0, 2), ncol = 2))
      return(result)
    }

    F_y = function(x, y){
      result = t(t(A_val %*% y - b_val) %*% A_val)
      return(result)
    }

    f_x = function(x, y){
      result = rbind(0, 0)
      return(result)
    }

    f_y = function(x, y){
      result = t(t(A_tr %*% y - b_tr) %*% A_tr)
      return(result)
    }

    g_x = function(x, y){
      result =  rbind(norm(y,type = "1"), 0.5*norm(y,type = "2")^2)
      return(result)
    }

    # Proximal operator for theta
    prox_eta = function(x, y, theta){
      z = theta - eta * (f_y(x, theta) + (theta - y) / gamma)
      lambda = eta * matrix(rep(x[1], p_fea), nrow = p_fea)
      result = sign(z) * pmax((abs(z) - eta * lambda), 0*ep) / (1+x[2]*eta)
    }

    # Proximal operator for y
    prox_beta = function(x, y, dky){
      z = y - beta * dky
      lambda = beta * matrix(rep(x[1], p_fea), nrow = p_fea)
      result = sign(z) * pmax((abs(z) - beta * lambda), 0*ep) / (1+x[2]*beta)
    }


    # Store the results
    X_seq = list()
    Y_seq = list()
    Theta_seq = list()
    F_seq = numeric(N)

    # Algorithm
    for (k in 1:N) {
      xk = x
      yk = y
      thetak = theta
      ck = c*(k)^power
      theta = prox_eta(x, y, theta)
      dkx = (1/ck) * F_x(x, y) + f_x(x, y) + g_x(x, y) - f_x(x, theta) - g_x(x, theta)
      x = pmax(x - alpha * dkx,0*e2)
      dky = (1/ck) * F_y(x, y) + f_y(x, y) - (y - theta)/gamma
      y = prox_beta(x, y, dky)

      X_seq[[k]] = x
      Y_seq[[k]] = y
      Theta_seq[[k]] = theta
      F_seq[k] = F(x, y)
      # conv_criterion = sqrt( norm(x - xk , "2")^2 + norm(y - yk, "2")^2 + norm(theta - thetak, "2")^2 ) / sqrt( 1 + norm(xk , "2")^2 + norm(yk, "2")^2 + norm(thetak, "2")^2 )
      conv_criterion = sqrt( norm(x - xk , "2")^2 + norm(y - yk, "2")^2) / sqrt( 1 + norm(xk , "2")^2 + norm(yk, "2")^2)
      if (!is.na(conv_criterion) && conv_criterion < tol) {
        if(auto_tuning == FALSE){
          cat("Terminating at iteration", k, "\n")
        }
        F_seq = F_seq[1:k]
        break
      }
    }

    return(list(x = x, y = y, X_seq = X_seq, Y_seq = Y_seq, F_seq = F_seq))
  }

  if(auto_tuning == TRUE){
    message("\n","Auto-hyperparameters-tuning is proceeding now.")

    iter <- 100
    T <- temperature

    pb <- progress_bar$new(
      total = iter,
      format = "  Finished :current/:total [:bar] :percent  remaining time :eta"
    )


    alpha.seq <- numeric(iter)
    beta.seq <- numeric(iter)
    eta.seq <- numeric(iter)
    value <- numeric(iter)

    alpha.seq[1] <- alpha
    beta.seq[1] <- beta
    eta.seq[1] <- eta

    result = main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[1], beta = beta.seq[1], eta = eta.seq[1], gamma = gamma, c = c, power = power, tol = tol)
    value[1] <- result$F_seq[order(result$F_seq, decreasing = FALSE)[1]]



    set.seed(123)
    for (j in 2:iter) {
      T <- exp(-0.0001*j)
      alpha.seq[j] <- rtruncnorm(n = 1, a = 0, mean = alpha.seq[j-1], sd = 1e-3)
      beta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = beta.seq[j-1], sd = 1e-6)
      eta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = eta.seq[j-1], sd = 1e-6)
      result = main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[j], beta = beta.seq[j], eta = eta.seq[j], gamma = gamma, c = c, power = power, tol = tol)
      candidate <- result$F_seq[order(result$F_seq, decreasing = FALSE)[1]]
      if(candidate > value[j-1] & runif(n = 1) > exp((value[j-1]-candidate)/T)){
        value[j] <- value[j-1]
      } else {
        value[j] <- candidate
      }
      pb$tick()
    }

    opt_index <- order(value)[1]

    cat("\n", "Auto-hyperparameters-tuning is done.")
    cat("\nFinal hyper-paramaters (alpha,beta,eta) are chosen as:",c(alpha.seq[opt_index], beta.seq[opt_index], eta.seq[opt_index]))

    return(main_fun(A_val, b_val, A_tr, b_tr, N, alpha = alpha.seq[opt_index], beta = beta.seq[opt_index], eta = eta.seq[opt_index], gamma, c, power, tol))

  } else{
    return(main_fun(A_val, b_val, A_tr, b_tr, N, alpha, beta, eta, gamma, c, power, tol))
  }
}
