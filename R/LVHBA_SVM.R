#' Solving support vector machine hyper-parameter selection problem based on LV-HBA
#' @description This R function is written to solve the hyper-parameter selection problem of Support Vector Machines (SVM) using the LV-HBA,
#'     please refer to the listed literature for the specific algorithm and model. It should be noted that in order to successfully apply the algorithm,
#'     the upper objective function is smoothed using softplus function approximation,
#'     and the lower problem is smoothed by introducing auxiliary variables and additional constraints.
#' @references Yao, W., Yu, C., Zeng, S., & Zhang, J. (2024).
#'     "Constrained Bi-Level Optimization: Proximal Lagrangian Value Function Approach and Hessian-free Algorithm."
#'     Available at: https://openreview.net/forum?id=xJ5N8qrEPl
#'
#'     Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
#'     "Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
#'     Available at: https://proceedings.mlr.press/v162/gao22j.html
#' @param features The feature matrix with dimensions n by p, where the j-th column corresponds to the j-th feature.
#' @param labels The label vector where the j-th element represents the label of the j-th feature,
#'     with a value of 1 indicating the positive class and -1 indicating the negative class.
#' @param K K-fold cross validation. Defult is 3.
#' @param N Total iterations. Default is 200.
#' @param alpha Projection stepsize in LV-HBA of \code{x,y} which is fixed. Default is 0.002.
#' @param beta Projection stepsize in LV-HBA of \code{z} which is fixed. Default is 0.002.
#' @param eta Projection stepsize  in LV-HBA of \code{theta, lambda} which is fixed. Default is 0.003.
#' @param gamma1 Proximal parameter in LV-HBA. Default is 1.
#' @param gamma2 Proximal parameter in LV-HBA. Default is 1.
#' @param c_0 which is used to generate the penalty parameter \eqn{c_k} in LV-HBA by \eqn{c_k = \underline{c}(k+1)^{power}} where \eqn{\underline{c} = c_0}.
#'     Default is 2.
#' @param power The power exponent in \eqn{c_k = \underline{c}(k+1)^power}. Default is 0.48.
#' @param r parameter in LV-HBA. Default is 10.
#' @param ub \eqn{\bar{w}_{ub}=ub*\mathbf{1}}, where \eqn{\mathbf{1}} is a vector of ones with the same size as  \eqn{\bar{w}}. Default is 10.
#' @param lb \eqn{\bar{w}_{lb}=lb*\mathbf{1}}, where \eqn{\mathbf{1}} is a vector of ones with the same size as  \eqn{\bar{w}}. Default is \eqn{10^{-6}}.
#' @param tol Tolerance. IF \eqn{|(x^{k + 1}, y^{k + 1}) - (x^{k}, y^k)|/\sqrt{1+|(x^{k}, y^k)|^2} < tol},
#'     then terminate the iteration, where \eqn{x^k} represents the upper-level variable, \eqn{y^k} represents the lower-level variable. Default is 0.001.
#' @param auto_tuning Whether an auto-hyperparameter-tuning is needed. Default is
#'     \code{FALSE}.
#' @param temperature Temperature of simulating annealing method for auto-
#'     hyper-parameter-tuning. Default is 0.1.
#'
#' @return
#'
#'    \item{x}{The optimal hyper-parameters, which is a vector of length (p+1).
#'        The first value is regularization parameter, and the remaining element stands for box constraint \eqn{\bar{w}}}
#'    \item{y}{A vector combines by variables and auxiliary variables of the subproblems.}
#'    \item{w}{A list of t vectors.
#'        The t-th element of the list \code{w} (which is a vector),
#'        and the t-th value of c (which is a scalar) represents the variables of the t-th subproblem.}
#'    \item{c}{A vector of t elements.
#'        The t-th element of the list \code{w} (which is a vector), and the t-th value of c (which is a scalar) represents the variables of the t-th subproblem.}
#'    \item{xi}{Auxiliary variable}
#'    \item{X_seq}{A list. Describe the sequence x generated by MEHA in the iterative process.}
#'    \item{Y_seq}{A list. Describe the sequence y generated by MEHA in the iterative process.}
#'    \item{F_seq}{The upper function value sequence generated in the iterative process.}
#'    \item{val_error}{The upper function value in the last iteration.}
#' @export



LVHBA_SVM <- function(features,
                      labels,
                      K = 3,
                      N = 200,
                      alpha = 0.002,
                      beta = 0.002,
                      eta = 0.003,
                      gamma1 = 1,
                      gamma2 = 1,
                      c_0 = 2,
                      power = 0.48,
                      r = 10,
                      ub = 10,
                      lb = 10^(-6),
                      tol = 1e-3,
                      auto_tuning = FALSE,
                      temperature = 0.1
){

  # Package used for solving projection steps
  library(quadprog)
  # Package used for concatenating subproblem constraints in the lower layer, to obtain matrices suitable for computation
  library(Matrix)
  library(progress)
  library(truncnorm)

  main_fun = function(features,
                      labels,
                      K,
                      N,
                      alpha,
                      beta,
                      eta,
                      gamma1,
                      gamma2,
                      c_0,
                      power,
                      r,
                      ub,
                      lb,
                      tol
  ){
    if(auto_tuning == FALSE){
      cat("Fold:", K, "\n")
    }

    if (ncol(features) != ncol(labels)) {
      stop('the number of samples and labels should be iedntity!')
    }
    num_cols = ncol(features)
    if(auto_tuning == FALSE){
      cat("Number of samples in the cross-validation set:", num_cols, "\n")
    }

    A = as.matrix(features)
    b = t(as.matrix(labels))


    data_index = 1:dim(A)[2]

    p = dim(A)[1]

    if(auto_tuning == FALSE){
      cat("Dimension of the sample vector p:", p, "\n")
    }


    wub = ub * matrix(1, p, 1)
    wlb = lb * matrix(1, p, 1)


    data_index_copy = data_index
    # K-fold, set size
    tr_set_size = length(data_index) %/% K
    if (tr_set_size < 1) {
      stop('the data can\'t be divided into K groups\n')
    }


    Omega_tr = list()
    Omega_val = list()


    for (i in 1:(K - 1)) {
      tr_set_indices = sample(length(data_index), tr_set_size, replace = FALSE)
      Omega_tr[[i]] = data_index[tr_set_indices]
      Omega_val[[i]] = setdiff(data_index_copy, Omega_tr[[i]])
      data_index = data_index[-tr_set_indices]
    }

    Omega_tr[[K]] = data_index
    Omega_val[[K]] = setdiff(data_index_copy, Omega_tr[[K]])

    Omega_tr_size = matrix(rep(1), K)
    Omega_val_size = matrix(rep(1), K)

    # Initial value
    lambda = 10
    w_bar = 10^(-6) * matrix(rep(1), nrow = p )
    c = matrix(rep(1), K)
    w = list()
    xi = list()
    E_xi = list()
    E_p = diag(1, p, p)
    ones_p = matrix(1, p, 1)
    for (t in 1:K) {
      w[[t]] = matrix(0,nrow = p, ncol = 1 )
      Omega_tr_size[t] = length(Omega_tr[[t]])
      Omega_val_size[t] = length(Omega_val[[t]])
      xi[[t]] = matrix(0,nrow = Omega_tr_size[t], ncol = 1)
      E_xi[[t]] = diag(1, Omega_tr_size[t], Omega_tr_size[t])
    }

    # x = (lambda, w_bar), y = (...,w^t, c^t, \xi^t,...), t = 1,...,K
    x = rbind(lambda,w_bar)
    y = matrix(1, nrow = K + p * K + ncol(A), ncol = 1)
    theta = matrix(1, nrow = K + p * K + ncol(A), ncol = 1)

    index_y = 0
    for (t in 1:K) {
      y[(index_y + 1):(index_y + p),] = w[[t]]
      y[index_y + p + 1,] = c[t]
      y[(index_y + p + 1 + 1):(index_y + p + 1 + Omega_tr_size[t]),] = xi[[t]]
      index_y = index_y + p + 1 + Omega_tr_size[t]
    }
    if(auto_tuning == FALSE){
      cat("The dimension of upper-level variable：", dim(x)[1], "\n")
      cat("The dimension of lower-level variable：", dim(y)[1], "\n")
    }


    S_x2 = matrix(0, nrow = 0, ncol = p)
    S_y = matrix(0, nrow = 0, ncol = 0)
    L = rbind(0, wub, -wlb)
    L_g = 0 * rbind(0, wub, -wlb)
    for (t in 1:K) {
      B_t = matrix(0, nrow = 0, ncol = p)
      b_t = matrix(0, nrow = 0, ncol = 1)
      for (j in Omega_tr[[t]]) {
        B_t = rbind(B_t, -b[j] * t(A[,j]))
        b_t = rbind(b_t, b[j])
      }

      A_t1 = cbind(-E_p, matrix(0, nrow = p, ncol = 1), matrix(0, nrow = p, ncol = ncol(E_xi[[t]])))
      A_t2 = -A_t1
      A_t3 = cbind(B_t, b_t, -E_xi[[t]])
      A_t4 = cbind(matrix(0, nrow = nrow(E_xi[[t]]), ncol = p), matrix(0, nrow = nrow(E_xi[[t]]), ncol = 1), -E_xi[[t]])
      A_t = rbind(A_t1, A_t2, A_t3, A_t4)
      S_y = bdiag(S_y, A_t)
      M_t = rbind(-E_p, -E_p, matrix(0, nrow = nrow(E_xi[[t]]), ncol = p), matrix(0, nrow = nrow(E_xi[[t]]), ncol = p))
      S_x2 = rbind(S_x2, M_t)
      L = rbind(L, matrix(0, nrow = p, ncol = 1), matrix(0, nrow = p, ncol = 1), matrix(-1, nrow = nrow(E_xi[[t]]), ncol = 1), matrix(0, nrow = nrow(E_xi[[t]]), ncol = 1))
      L_g = rbind(L_g, matrix(0, nrow = p, ncol = 1), matrix(0, nrow = p, ncol = 1), matrix(-1, nrow = nrow(E_xi[[t]]), ncol = 1), matrix(0, nrow = nrow(E_xi[[t]]), ncol = 1))
    }


    S_x1 = matrix(-1, nrow = 1, ncol = 1) # Upper-left submatrix S_x1 of S
    # Concatenate the lower-right submatrix S_x2y, corresponding to the coefficient matrix for (w_bar, y)
    S_x2y_12 = cbind( rbind(E_p, -E_p), matrix(0, 2 * p, K + p * K + ncol(A)))

    S = as.matrix( bdiag(S_x1, rbind(S_x2y_12, cbind(S_x2, S_y) )) )

    S_g = as.matrix( bdiag(0 * S_x1, rbind(0 * S_x2y_12, cbind(S_x2, S_y) )) )


    # closed set Z
    U = matrix(rep(1), nrow = nrow(S))*r
    z = matrix(rep(1), nrow = nrow(S))
    lam = matrix(rep(1), nrow = nrow(S)) # lambda in LV-HVA

    # theta
    theta_w = w
    theta_c = c
    theta_xi = xi


    # Approximate upper-level objective function by softplus
    up_fun = function(lambda, w_bar, w, c, xi){
      result = 0
      for (t in 1:K) {
        for (j in Omega_val[[t]]) {
          result = result + (1/Omega_val_size[t]) * log( 1 + exp(1 - b[j] * (t(A[, j]) %*% w[[t]] - c[t]) ))
        }
      }
      return((1/K)*result)
    }

    # Upper-level objective function
    up_fun_real = function(lambda,w_bar, w, c, xi){
      result = 0
      for (t in 1:K) {
        for (j in Omega_val[[t]]) {
          result = result + (1/Omega_val_size[t]) * max(1 - b[j] * (t(A[, j]) %*% w[[t]] - c[t]),0)
        }
      }
      return((1/K)*result)
    }


    # Lower-level objective function
    low_fun = function(lambda, w_bar, w, c, xi){
      result = 0
      for (t in 1:K) {
        result = result + 0.5 * lambda * norm(w[[t]],type = 2)^2 + sum(xi[t])
      }
    }

    # Constrain function
    g = function(lambda, w_bar, w, c, xi){
      x = rbind(lambda, w_bar)
      y = matrix(0, nrow = 0, ncol = 1)
      for (t in 1:K) {
        y = rbind(y, as.matrix(w[[t]]), c[t], as.matrix(xi[[t]]))
      }
      x_y = rbind(x, y)
      return(S_g %*% x_y - L_g)
    }


    # Gradient update function for x and y
    F_x = function(lambda, w_bar, w, c, xi){
      result = matrix(0, p + 1, 1)
      return(result)
    }

    F_y <- function(lambda, w_bar, w, c, xi) {
      # Initialize gradients for w and c
      grad_w = list()
      grad_c = matrix(0, K, 1)
      grad_xi = list()
      result = matrix(0, 0, 1)

      # Calculate the partial derivatives
      for (t in 1:K) {
        grad_w[[t]] = 0 * ones_p
        for (j in Omega_val[[t]]) {
          exp_term = exp(1 - b[j] * (t(A[,j]) %*% w[[t]] - c[t]))
          common_term = exp_term / (1 + exp_term)
          grad_w[[t]] = grad_w[[t]] - (1 / K) * (1 / length(Omega_val[[t]])) * as.numeric(common_term * b[j]) * A[,j]
          grad_c[t] = grad_c[t] + (1 / K) * (1 / length(Omega_val[[t]])) * common_term * b[j]
        }
        grad_xi[[t]] = matrix(rep(0),nrow = Omega_tr_size[t])
        result = rbind(result, grad_w[[t]], grad_c[t], grad_xi[[t]])
      }
      return(result)
    }

    f_x = function(lambda, w_bar, w, c, xi){
      result = matrix(0, p + 1, 1)
      return(result)
    }

    f_y = function(lambda, w_bar, w, c, xi){
      grad_w = list()
      grad_c = matrix(0, K, 1)
      grad_y = matrix(0, 0, 1)
      grad_xi = list()
      for (t in 1:K) {
        grad_w[[t]] = lambda * w[[t]]
        grad_c[t] = 0
        grad_xi[[t]] = matrix(1,nrow = Omega_tr_size[t], ncol = 1)
        grad_y = rbind(grad_y, as.matrix(grad_w[[t]]), grad_c[t],grad_xi[[t]])
      }
      return(list(grad_w = grad_w, grad_c = grad_c, grad_xi = grad_xi, grad_y = grad_y))
    }


    g_x = function(lambda, w_bar, theta_w, theta_c, theta_xi){
      result = S_g[, 1:(p + 1)]
      return(t(result))
    }

    g_y = function(lambda, w_bar, theta_w, theta_c, theta_xi){
      result = S_g[, (p + 2):ncol(S_g)]
      return(t(result))
    }

    # Store the results
    X_seq = list()
    Y_seq = list()
    Theta_seq = list()
    F_seq = numeric(N)

    # Iteratiin
    for (k in 1:N) {
      xk = x
      yk = y
      thetak = theta
      ck = c_0 * (k)^power
      grad_f_y = f_y(lambda, w_bar, theta_w, theta_c, theta_xi)
      dk_theta = grad_f_y$grad_y + g_y(lambda, w_bar, theta_w, theta_c, theta_xi) %*% lam + (theta - y) / gamma1
      dk_lam = -g(lambda, w_bar, theta_w, theta_c, theta_xi) + (lam - z) / gamma2

      theta = theta - (eta * dk_theta)
      theta_w <- list()
      theta_c <- matrix(rep(0), nrow = K)
      theta_xi = list()
      # Update theta
      index_theta = 0
      for (t in 1:K) {
        theta_w[[t]] = theta[(index_theta + 1):(index_theta + p), , drop = FALSE]
        theta_c[t] = theta[index_theta + p + 1, , drop = FALSE]
        theta_xi[[t]] = theta[(index_theta + p + 1 + 1):(index_theta + p + 1 + Omega_tr_size[t]), , drop = FALSE]
        index_theta = index_theta + p + 1 + Omega_tr_size[t]
      }

      lam = lam - (eta * dk_lam)
      lam = pmin(pmax(lam, 0), U)

      dkx = F_x(lambda, w_bar, w, c, xi) / ck + f_x(lambda, w_bar, w, c, xi) - f_x(lambda, w_bar, theta_w, theta_c, theta_xi) - g_x(lambda, w_bar, theta_w, theta_c, theta_xi) %*% lam
      dky = F_y(lambda, w_bar, w, c, xi) / ck + f_y(lambda, w_bar, w, c, xi)$grad_y - (y - theta) / gamma1
      x = x - alpha * dkx
      y = y - alpha * dky
      x_y = rbind(x, y)
      # Solve the projection
      x_y = as.matrix(solve.QP(Dmat = diag(1, ncol(S), ncol(S)), dvec =  x_y, Amat = t(-S), bvec = -L)$solution)

      x = x_y[1:(1 + p), , drop = FALSE]
      # Update the lower-level variables lambda and w_bar
      lambda = x[1]
      w_bar = x[2:(p + 1), , drop = FALSE]

      y = x_y[(2 + p):length(x_y), ,drop = FALSE]


      index_y = 0
      for (t in 1:K) {
        w[[t]] = y[(index_y + 1):(index_y + p), , drop = FALSE]
        c[t] = y[index_y + p + 1, , drop = FALSE]
        xi[[t]] = y[(index_y + p + 1 + 1):(index_y + p + 1 + Omega_tr_size[t]), , drop = FALSE]
        index_y = index_y + p + 1 + Omega_tr_size[t]
      }


      dkz = -(lam - z) / gamma2
      z = z - (beta * dkz)
      z = pmin(pmax(z, 0),U)


      X_seq[[k]] = x
      Y_seq[[k]] = y
      Theta_seq[[k]] = theta
      F_seq[k] = up_fun_real(lambda, w_bar, w, c, xi)

      #conv_criterion = sqrt( norm(x - xk , "2")^2 + norm(y - yk, "2")^2 + norm(theta - thetak, "2")^2 ) / sqrt( 1 + norm(xk , "2")^2 + norm(yk, "2")^2 + norm(theta, "2")^2 )
      conv_criterion = sqrt( norm(x - xk , "2")^2 + norm(y - yk, "2")^2 ) / sqrt( 1 + norm(xk , "2")^2 + norm(yk, "2")^2  )
      if (!is.na(conv_criterion) && conv_criterion < tol) {
        if(auto_tuning == FALSE){
          cat("Terminating at iteration", k, "\n")
        }
        F_seq = F_seq[1:k]
        break
      }
    }

    val_error = up_fun_real(lambda, w_bar, w, c, xi)

    return(list(x = x, y = y, w = w, c = c, xi = xi, X_seq = X_seq, Y_seq = Y_seq, F_seq = F_seq, val_error = val_error))
  }


  if(auto_tuning == TRUE){
    message("\n","Auto-hyperparameters-tuning is proceeding now.")
    iter <- 100
    pb <- progress_bar$new(
      total = iter,
      format = "  Finished :current/:total [:bar] :percent  remaining time :eta"
    )
    alpha.seq <- numeric(iter)
    beta.seq <- numeric(iter)
    eta.seq <- numeric(iter)
    value <- numeric(iter)

    alpha.seq[1] <- alpha
    beta.seq[1] <- beta
    eta.seq[1] <- eta

    result <- main_fun(features,
                       labels,
                       K,
                       N,
                       alpha = alpha.seq[1],
                       beta = beta.seq[1],
                       eta = eta.seq[1],
                       gamma1 = gamma1,
                       gamma2 = gamma2,
                       c_0 = c_0,
                       power= power,
                       r = r,
                       ub = ub,
                       lb = lb,
                       tol = tol
    )

    value[1] <- result$val_error[order(result$val_error, decreasing = FALSE)[1]]

    set.seed(123)
    for (j in 2:iter) {
      #T <- T*exp(-0.01*j)
      alpha.seq[j] <- rtruncnorm(n = 1, a = 0, mean = alpha.seq[j-1], sd = 1e-3)
      beta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = beta.seq[j-1], sd = 1e-6)
      eta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = eta.seq[j-1], sd = 1e-6)
      result <-   main_fun(features,
                           labels,
                           K,
                           N,
                           alpha = alpha.seq[j],
                           beta = beta.seq[j],
                           eta = eta.seq[j],
                           gamma1 = gamma1,
                           gamma2 = gamma2,
                           c_0 = c_0,
                           power = power,
                           r = r,
                           ub = ub,
                           lb = lb,
                           tol = tol
      )
      candidate <- result$val_error[order(result$val_error, decreasing = FALSE)[1]]

      if(candidate > value[j-1] & runif(n = 1) > exp((value[j-1]-candidate)/temperature)){
        value[j] <- value[j-1]
      } else {
        value[j] <- candidate
      }
      pb$tick()
    }
    opt_index <- order(value)[1]

    cat("\n", "Auto-hyperparameters-tuning is done.")
    cat("\nFinal hyper-paramaters (alpha,beta,eta) are chosen as:",c(alpha.seq[opt_index], beta.seq[opt_index], eta.seq[opt_index]))
    return(main_fun(features,
                    labels,
                    K,
                    N,
                    eta = eta.seq[opt_index],
                    alpha = alpha.seq[opt_index],
                    beta = beta.seq[opt_index],
                    gamma1 = gamma1,
                    gamma2 = gamma2,
                    c_0 = c_0,
                    power = power,
                    r = r,
                    ub = ub,
                    lb = lb,
                    tol = tol
                    )
           )
  }
  else{
    main_fun(features, labels, K, N, eta, alpha, beta, gamma1 = gamma1, gamma2 = gamma2, c_0 = c_0, power = power, r = r, ub = ub, lb = lb, tol = tol
    )
  }
}
