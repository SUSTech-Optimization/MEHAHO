#' Solving sparse group lasso hyper-parameter selection problem based on MEHA
#' @description This R function is written to solve the hyper-parameter selection problem of sparse group lasso using MEHA,
#'     please refer to the listed literature for the specific algorithm and model.
#' @references Liu, R., Liu, Z., Yao, W., Zeng, S., & Zhang, J. (2024).
#'     "Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy."
#'     Available at: https://openreview.net/forum?id=i6EtCiIK4a
#'
#'     Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
#'     "Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
#'     Available at: https://proceedings.mlr.press/v162/gao22j.html
#' @param A_val Input feature matrix of validation set, each row of which
#'     is an observation vector.
#' @param b_val Quantitative response variable of validation set.
#' @param A_tr Input feature matrix of training set.
#' @param b_tr Quantitative response variable of training set.
#' @param group A vector of length M (where M denotes the total group number) to
#'     describe the feature group information, with each element representing the
#'     specific number of features in each group.
#'     For example, if the coefficients of dimension 12 are divided into 3 groups which is 6, 3, and 3 dimensions,
#'     then \code{group=(6,3,3)}. In the case of non-sequential grouping, the permutation matrix can be converted to sequential grouping.
#' @param N Total iterations. Default is 500.
#' @param alpha Projection stepsize of \code{x}. Default is 1e-4 which is fixed.
#'     Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.
#' @param beta Proximal gradient stepsize of \code{y}.
#'     Default is 1e-3 which is fixed.
#'     Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.
#' @param eta Proximal gradient stepsize of the proxima \code{theta}.
#'     Default is 1e-3 which is fixed.
#' @param gamma Moreau envelope parameter. Default is 1.
#' @param c \eqn{\underline{c}} in MEHA, which is used to generate the penalty parameter \eqn{c_k} by \eqn{c_k = \underline{c}(k+1)^{power}}.
#'     Default is 1.
#' @param power The power exponent in \eqn{c_k = \underline{c}(k+1)^{power}}. Default is 0.49.
#' @param tol Tolerance. IF \eqn{|(x^{k + 1}, y^{k + 1}) - (x^{k}, y^k)|/\sqrt{1+|(x^{k}, y^k)|^2} < tol},
#'     then terminate the iteration, where \eqn{x^k} represents the upper-level variable, \eqn{y^k} represents the lower-level variable.
#'     Default is 0.05.
#' @param auto_tuning When alpha, beta, eta are fixed, whether an auto-hyperparameter-tuning is needed.
#'     Default is \code{FALSE}.
#' @param temperature Temperature of simulating annealing method for auto-
#'     hyperparameter-tuning. Default is 0.01.
#'
#' @return
#'
#'   \item{x}{The optimal regularization parameters, which is a vector of length (M+1). M denotes the total group number.
#'       The first M values are the corresponding coefficients of m-th l2-norm group penalty, while \code{x_{M+1}}
#'       value represents the coefficient of l1-norm penalty.}
#'   \item{y}{The feature coefficient vector, of dimension p, where p is the
#'       dimension of features.}
#'   \item{X_seq}{A list. Describe the sequence x generated by MEHA in the iterative process.}
#'   \item{Y_seq}{A list. Describe the sequence y generated by MEHA in the iterative process.}
#'   \item{F_seq}{The upper function value sequence generated in the iterative process based on validation set.}
#'
#'
#' @export

MEHA_SGL = function(A_val, b_val, A_tr, b_tr, group, N = 500, alpha = 1e-4,
                    beta = 1e-3, eta = 1e-3, gamma = 1, c = 1, power = 0.49,tol = 0.05,
                    auto_tuning = FALSE, temperature = 0.01){

  library(progress)
  library(truncnorm)

  main_fun <- function(A_val, b_val, A_tr, b_tr, group,
                       N, alpha, beta, eta, gamma, c, power,tol){

    p_fea = dim(A_val)[2] # Dimension of features

    group_num = dim(group)[1]

    if (p_fea != sum(group)) {
      return(print("Error: p_fea != sum(group), he grouping condition contradicts the number of features"))
    }

    # Initial values
    x = matrix(rep(0.01), nrow = group_num + 1)
    y = matrix(rep(0), nrow = p_fea)
    theta = y


    e0 = matrix(rep(0), nrow = group_num + 1)
    ep = matrix(rep(0), nrow = p_fea)
    ep1 = matrix(rep(1), nrow = p_fea)


    # Upper objective function
    up_fun = function(x,y){
      result = 0.5*norm(A_val %*% y - b_val, type = "2")^2
      return(result)
    }

    fun_group = function(x,y){
      result = 0
      for (k in 1:group_num){
        result = x[k]*norm( y[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] ,type = "2") + result
      }
      return(result)
    }

    low_fun = function(x, y){
      result = 0.5*norm(A_tr %*%  y - b_tr, type = "2")^2 + fun_group(x,y) + x[group_num + 1]* norm( y ,type = "1")
      return(result)
    }


    # Gradient update function for x and y
    F_x = function(x, y){
      result = e0
      return(result)
    }

    F_y = function(x, y){
      result = t(t(A_val %*% y - b_val) %*% A_val)
      return(result)
    }

    f_x = function(x, y){
      result = e0
      return(result)
    }

    f_y = function(x, y){
      result = t(t(A_tr %*% y - b_tr) %*% A_tr)
      return(result)
    }

    g_x = function(x, y){
      result = e0
      for (k in 1:group_num){
        yk = y[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])]
        result[k] = norm( yk ,type = "2")
      }
      result[group_num + 1] = norm( y ,type = "1")
      return(result)
    }


    # Proximal operator for theta
    # Formula method reference
    # Zhang, Y., Zhang, N., Sun, D., & Toh, K.-C. (2020).
    # An efficient Hessian based algorithm for solving large-scale sparse group Lasso problems.
    # Mathematical Programming, 179(1-2), 223–263.
    # https://doi.org/10.1007/s10107-018-1329-6
    prox_eta = function(x, y, theta){
      z = theta - eta * (f_y(x, theta) + (theta - y) / gamma)
      z2 = ep
      result = ep
      for (k in 1:group_num) {
        zk = z[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])]
        ek = ep1[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])]

        if (eta * x[group_num + 1] > 0) {
          z2[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] = sign(zk) * pmax((abs(zk) - eta * x[group_num + 1] * ek), 0*ek)
        }
        else{
          z2[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] = zk
        }
        z2k = z2[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])]

        if (eta * x[k] > 0) {
          result[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] = (1 - eta * x[k]/(max(norm(z2k,type = "2"), eta * x[k])))*z2k
        }
        else {
          result[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] = z2k
        }
      }
      return(result)
    }

    # Proximal operator for y
    # Formula method reference
    # Zhang, Y., Zhang, N., Sun, D., & Toh, K.-C. (2020).
    # An efficient Hessian based algorithm for solving large-scale sparse group Lasso problems.
    # Mathematical Programming, 179(1-2), 223–263.
    # https://doi.org/10.1007/s10107-018-1329-6
    prox_beta = function(x, y, dky){
      z = y - beta * dky
      z2 = ep
      result = ep
      for (k in 1:group_num) {
        zk = z[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])]
        ek = ep1[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])]

        if (beta * x[group_num + 1] > 0) {
          z2[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] = sign(zk) * pmax((abs(zk) - beta * x[group_num + 1] * ek), 0*ek)
        }
        else{
          z2[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] = zk
        }
        z2k = z2[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])]

        if (beta * x[k] > 0) {
          result[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] = (1 - beta * x[k]/(max(norm(z2k,type = "2"), beta * x[k])))*z2k
        }
        else {
          result[(sum(group[1:k]) - group[k] + 1):sum(group[1:k])] = z2k
        }
      }
      return(result)
    }


    # Store the results
    X_seq = list()
    Y_seq = list()
    Theta_seq = list()
    F_seq = numeric(N)

    # Iteration
    for (k in 1:N) {
      xk = x
      yk = y
      thetak = theta

      ck = c*(k)^power
      theta = prox_eta(x, y, theta)

      dkx = (1/ck) * F_x(x, y) + f_x(x, y) + g_x(x, y) - f_x(x, theta) - g_x(x, theta)
      x = pmax(x - alpha * dkx,0*e0)

      dky = (1/ck) * F_y(x, y) + f_y(x, y) - (y - theta)/gamma
      y = prox_beta(x, y, dky)


      X_seq[[k]] = x
      Y_seq[[k]] = y
      Theta_seq[[k]] = theta
      F_seq[k] = up_fun(x, y)

      #conv_criterion = sqrt( norm(x - xk , "2")^2 + norm(y - yk, "2")^2 + norm(theta - thetak, "2")^2 ) / sqrt( 1 + norm(xk , "2")^2 + norm(yk, "2")^2 + norm(thetak, "2")^2 )
      conv_criterion = sqrt( norm(x - xk , "2")^2 + norm(y - yk, "2")^2 ) / sqrt( 1 + norm(xk , "2")^2 + norm(yk, "2")^2  )
      if (!is.na(conv_criterion) && conv_criterion < tol) {
        if(auto_tuning == FALSE){
          cat("Terminating at iteration", k, "\n")
        }
        F_seq = F_seq[1:k]
        break
      }

    }
    return(list(x = x, y = y, X_seq = X_seq, Y_seq = Y_seq, F_seq = F_seq))
  }


  if(auto_tuning == TRUE){
    message("\n","Auto-hyperparameters-tuning is proceeding now.")

    iter <- 100
    T <- temperature

    pb <- progress_bar$new(
      total = iter,
      format = "  Finished :current/:total [:bar] :percent  remaining time :eta"
    )


    alpha.seq <- numeric(iter)
    beta.seq <- numeric(iter)
    eta.seq <- numeric(iter)
    value <- numeric(iter)

    alpha.seq[1] <- alpha
    beta.seq[1] <- beta
    eta.seq[1] <- eta

    result <- main_fun(A_val, b_val, A_tr, b_tr, group,N, alpha = alpha.seq[1], beta = beta.seq[1], eta = eta.seq[1], c = c, gamma = gamma, power = power, tol = tol)
    value[1] <- result$F_seq[order(result$F_seq, decreasing = FALSE)[1]]



    set.seed(123)
    for (j in 2:iter) {
      #T <- T*exp(-0.01*j)
      alpha.seq[j] <- rtruncnorm(n = 1, a = 0, mean = alpha.seq[j-1], sd = 1e-3)
      beta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = beta.seq[j-1], sd = 1e-6)
      eta.seq[j] <- rtruncnorm(n = 1, a = 0, mean = eta.seq[j-1], sd = 1e-6)
      result <- main_fun(A_val, b_val, A_tr, b_tr, group,N, alpha = alpha.seq[j], beta = beta.seq[j], eta = eta.seq[j], c = c, gamma = gamma, power = power, tol = tol)
      candidate <- result$F_seq[order(result$F_seq, decreasing = FALSE)[1]]
      if(candidate > value[j-1] & runif(n = 1) > exp((value[j-1]-candidate)/T)){
        value[j] <- value[j-1]
      } else {
        value[j] <- candidate
      }
      pb$tick()
    }


    opt_index <- order(value)[1]

    cat("\n", "Auto-hyperparameters-tuning is done.")
    cat("\nFinal hyper-paramaters (alpha,beta,eta) are chosen as:",c(alpha.seq[opt_index], beta.seq[opt_index], eta.seq[opt_index]))

    return(main_fun(A_val, b_val, A_tr, b_tr, group, N, alpha = alpha.seq[opt_index], beta = beta.seq[opt_index], eta = eta.seq[opt_index], gamma, c, power, tol))
  }

  else{
    return(main_fun(A_val, b_val, A_tr, b_tr, group, N, alpha, beta, eta, gamma, c, power, tol))

  }


}
