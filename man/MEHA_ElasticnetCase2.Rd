% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MEHA_ElasticNetCase2.R
\name{MEHA_ElasticnetCase2}
\alias{MEHA_ElasticnetCase2}
\title{Solving elastic net hyper-parameter selection problem Based on MEHA}
\usage{
MEHA_ElasticnetCase2(
  A_val,
  b_val,
  A_tr,
  b_tr,
  N = 500,
  alpha = 0.001,
  beta = 1e-05,
  eta = 1e-05,
  gamma = 2,
  c = 10,
  power = 0.48,
  tol = 0.05,
  auto_tuning = FALSE,
  temperature = 0.1
)
}
\arguments{
\item{A_val}{Input feature matrix of the validation set, with dimensions n by p,
where n is the total number of validation samples and p is the dimension of
features. Each row represents an observation vector.}

\item{b_val}{Quantitative response variable of validation set.}

\item{A_tr}{Input feature matrix of training set, with dimension n' by p,
where n' is the total number of training samples and p is the dimension of features.}

\item{b_tr}{Quantitative response variable of training set.}

\item{N}{Total iterations of MEHA. Default is 500.}

\item{alpha}{Projection stepsize of \code{x}, which is fixed. Default is 0.001.
Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.}

\item{beta}{Proximal gradient stepsize of \code{y} which is fixed. Default is 1e-5.
Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.}

\item{eta}{Proximal gradient stepsize of \code{theta}, which is fixed.
Default is 1e-5.}

\item{gamma}{Moreau envelope parameter. Default is 2.}

\item{c}{\eqn{\underline{c}} in MEHA, which is used to generate the penalty parameter \eqn{c_k} by \eqn{c_k = \underline{c}(k+1)^{power}}.
Default is 10.}

\item{power}{The power exponent in \eqn{c_k = \underline{c}(k+1)^{power}}. Default is 0.48.}

\item{tol}{Tolerance. IF \eqn{|(x^{k + 1}, y^{k + 1}) - (x^{k}, y^k)|/\sqrt{1+|(x^{k}, y^k)|^2} < tol},
then terminate the iteration, where \eqn{x^k} represents the upper-level variable, \eqn{y^k} represents the lower-level variable.
Default is 0.05.}

\item{auto_tuning}{When alpha, beta, eta are fixed, whether an auto-hyperparameter-tuning is needed.
Default is \code{FALSE}.}

\item{temperature}{Temperature of simulating annealing method for auto-
hyperparameter-tuning. Default is 0.1.}
}
\value{
\item{x}{The first value is \code{x1} (Lasso penalty strength), while the
second value is \code{x2} (Ridge penalty strength).}
\item{y}{The feature coefficient vector, of dimension p, where p is the
p is the dimension of features.}
\item{X_seq}{A list. Describe the sequence x generated by MEHA in the iterative process.}
\item{Y_seq}{A list. Describe the sequence y generated by MEHA in the iterative process.}
\item{F_seq}{The upper function value sequence generated in the iterative process based on validation set.}
}
\description{
This R function is written to solve the hyper-parameter selection problem of elastic net using the MEHA,
please refer to the listed literature for the specific algorithm and model. In this case, the 2-norm term (which is smooth) of lower-level objective function is divided into \eqn{f(x,y)} in the problem studied by MEHA.
}
\references{
Liu, R., Liu, Z., Yao, W., Zeng, S., & Zhang, J. (2024).
"Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy."
Available at: https://openreview.net/forum?id=i6EtCiIK4a

\if{html}{\out{<div class="sourceCode">}}\preformatted{Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
"Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
Available at: https://proceedings.mlr.press/v162/gao22j.html
}\if{html}{\out{</div>}}
}
