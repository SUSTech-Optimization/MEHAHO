% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MEHA_SGL.R
\name{MEHA_SGL}
\alias{MEHA_SGL}
\title{Solving sparse group lasso hyper-parameter selection problem based on MEHA}
\usage{
MEHA_SGL(
  A_val,
  b_val,
  A_tr,
  b_tr,
  group,
  N = 500,
  alpha = 1e-04,
  beta = 0.001,
  eta = 0.001,
  gamma = 1,
  c = 1,
  power = 0.49,
  tol = 0.05,
  auto_tuning = FALSE,
  temperature = 0.01
)
}
\arguments{
\item{A_val}{Input feature matrix of validation set, each row of which
is an observation vector.}

\item{b_val}{Quantitative response variable of validation set.}

\item{A_tr}{Input feature matrix of training set.}

\item{b_tr}{Quantitative response variable of training set.}

\item{group}{A vector of length M (where M denotes the total group number) to
describe the feature group information, with each element representing the
specific number of features in each group.
For example, if the coefficients of dimension 12 are divided into 3 groups which is 6, 3, and 3 dimensions,
then \code{group=(6,3,3)}. In the case of non-sequential grouping, the permutation matrix can be converted to sequential grouping.}

\item{N}{Total iterations. Default is 500.}

\item{alpha}{Projection stepsize of \code{x}. Default is 1e-4 which is fixed.
Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.}

\item{beta}{Proximal gradient stepsize of \code{y}.
Default is 1e-3 which is fixed.
Specifically, you can edit the code and utilize the inverse power learning rate annealing strategy to dynamically adjust it.}

\item{eta}{Proximal gradient stepsize of the proxima \code{theta}.
Default is 1e-3 which is fixed.}

\item{gamma}{Moreau envelope parameter. Default is 1.}

\item{c}{\eqn{\underline{c}} in MEHA, which is used to generate the penalty parameter \eqn{c_k} by \eqn{c_k = \underline{c}(k+1)^{power}}.
Default is 1.}

\item{power}{The power exponent in \eqn{c_k = \underline{c}(k+1)^{power}}. Default is 0.49.}

\item{tol}{Tolerance. IF \eqn{|(x^{k + 1}, y^{k + 1}) - (x^{k}, y^k)|/\sqrt{1+|(x^{k}, y^k)|^2} < tol},
then terminate the iteration, where \eqn{x^k} represents the upper-level variable, \eqn{y^k} represents the lower-level variable.
Default is 0.05.}

\item{auto_tuning}{When alpha, beta, eta are fixed, whether an auto-hyperparameter-tuning is needed.
Default is \code{FALSE}.}

\item{temperature}{Temperature of simulating annealing method for auto-
hyperparameter-tuning. Default is 0.01.}
}
\value{
\item{x}{The optimal regularization parameters, which is a vector of length (M+1). M denotes the total group number.
The first M values are the corresponding coefficients of m-th l2-norm group penalty, while \code{x_{M+1}}
value represents the coefficient of l1-norm penalty.}
\item{y}{The feature coefficient vector, of dimension p, where p is the
dimension of features.}
\item{X_seq}{A list. Describe the sequence x generated by MEHA in the iterative process.}
\item{Y_seq}{A list. Describe the sequence y generated by MEHA in the iterative process.}
\item{F_seq}{The upper function value sequence generated in the iterative process based on validation set.}
}
\description{
This R function is written to solve the hyper-parameter selection problem of sparse group lasso using MEHA,
please refer to the listed literature for the specific algorithm and model.
}
\references{
Liu, R., Liu, Z., Yao, W., Zeng, S., & Zhang, J. (2024).
"Rethinking Moreau Envelope for Nonconvex Bi-Level Optimization: A Single-loop and Hessian-free Solution Strategy."
Available at: https://openreview.net/forum?id=i6EtCiIK4a

\if{html}{\out{<div class="sourceCode">}}\preformatted{Gao, L., Ye, J. J., Yin, H., Zeng, S., & Zhang, J. (2022).
"Value function based difference-of-convex algorithm for bilevel hyperparameter selection problems."
Available at: https://proceedings.mlr.press/v162/gao22j.html
}\if{html}{\out{</div>}}
}
